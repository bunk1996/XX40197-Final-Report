\chapter{Discussion}

\section{Symbol Recovery Capabilities and Limitations}
\subsection{Real World Signal Characteristics}
The analysis of the real world data recordings highlight the robustness of MSK as a modulation method. The exemplar dataset used shows unusually high levels of lightning activity that can be clearly characterised as continuous and impulsive, through looking at the time series output of the developed noise estimation routine. Additionally the recovered baseband signals clearly show the expected underlying characteristics of an MSK signal.  
\\\\
When estimating the BER of these recorded signals there are two potential limitations. Firstly a double bit error relating to symbols with opposite signs can cancel out an error in the estimation process. A less prevalent issue is potentially caused by limitations of the unwrapping process, causing $2\pi$ jump leading to a very preventable bit loss. This can be easily avoided by shifting the phase so that turning points lie away from $\pm\pi$ boundary, if this still proves to be an issue. There are adaptive methods that can be recursively applied to locally move points away from the $\pm\pi$ boundary. Equally the effect of clipping means that the SNR estimate becomes somewhat arbitrary because the impact of the lightning is not equally represented across all the transmitters, as it should be.

\subsection{Simulation Tools}

The simulation tools provided a powerful tool to simulate the VLF transmissions in the recorded data. This was closely validated by comparing the output of the simulation. The designed simulator is capable of replicating noise distribution present in a dataset that it is intended to imitate. This does have to be implemented with additional gains in this particular instance because the data does not fully represent the magnitude of the lightning, due to limitations introduced by clipping during data acquisition. However, it does provide an excellent facility to assess the limitations of MSK and bit recovery.
\\\\
The techniques applied to implement this tool show their flexibility and usefulness. They produce the expected signals with the varying characteristics similar to those shown by the recorded data. Especially with regards to amplitude distribution and similar power spectral density.
\\\\
The primary contribution of this tool was to provide the ability to vary the nature of the interference present in the signal in order to identify what characteristics give rise to a bit error.

\subsection{Symbol Recovery}
The techniques used for symbol recovery show promising results, the algorithm utilised known properties of the signals and showed good results for all signals and promising results for error correction.
\\\\
There are two types of bit loss: The first is when the lightning event causes a premature transition, this is when there is an apparent phase change in the correct place, typically one bit time early. The other is when there is a single opposite symbol in a long sequence of symbol. The lightning can often cause these symbols to be missed. The errors typically occur on the trailing end of a lightning event see figure \ref{fig:noiseamp}.
\\\\
Although it is easy to quantify the bit error during simulation as the signal is known, because the ultimate objective is to maximise the number of bits recovered it is necessary to be able to isolate temporal position of the error. Several metrics have been proposed in order to isolate certain signal characteristics based on spacial proximity to a transition point and analysis of local area statistics. The output of these analyses provides thresholds which can be applied to help narrow down where in the time series a bit error is likely to have occurred.
\\\\
The comparison of SNR and BER proves only a useful metric when used exclusively within the two separate domains because the simulatedsignals appear to exhibit much more resilience to the impulsive noise than the real data. 
The estimated BER was much higher relative to the SNR in the simulated signals, than in the recorded data. This observation is of little surprise as the lightning is near instantaneous relative to the signal, the simulations have very intensive 'lightning' present and as previously established, not every lightning event causes a bit loss. Compared the work of \cite{Yang2016} a much higher resilience to SNR is shown. Much lower SNR values give equivalent bit recovery. This also depends on the nature of the lightning, because SNR is calculated based on signal power it is related to $|A|^2$. The high amplitude noise will have a greater impact on SNR than continuous noise but each has similar potential to cause a bit error compared to the relative effect on SNR. This assessment is exemplified in the comparison of bit recovery when applied to the two different real datasets, there isn't a large difference between the SNR of the two signals however one set is clearly contains much more interference. 
\\\\
The error correction implemented involved the development of two local area statistics from the data to investigate the signal properties at each calculated bit transition. Using a thresholding method, points where both metrics were satisfied were isolated. Around each of these points the phase was approximated to a straight line, this method overall proved to reduce the bit error rate. It was limited in instances where the points of interest occurred at an actual turning point of the phase. This has the effect of eliminating the discrete turning point in turn causing a bit rate. Despite this the overall effect of the system was positive. A full parameter study is required in order to optimise the thresholds. Secondly a straight line approximation is a very crude technique, a more suitable solution may be some form of filter or higher order approximation is likely to offer improvement of this technique.    

\section{Errors and Uncertainties}
\subsection{Real Signal Analysis}
The main area for potential error when analysing the real signal is related to the estimation of the BER, due to potential limitations in phase unwrapping and rounding errors in the calculation. Potentially this could result in a false estimation of the number of each symbol present in the message which has a knock on effect on the calculation of the BER. The main uncertainty present is the calculation of the noise present in the signal. The power of the transmitter is unknown, combined with the potential for propagation loss, the signal power and noise have to be estimated. Although the use of the previously described percentile method in section \ref{sec:investigations}, is a well used a fairly robust method the calculated results still has an inherent uncertainty associated with estimation.
\subsection{Simulation Tools}
The main area of uncertainty within the simulation is the accurate representation of lightning interference. Although best efforts were made to validate the simulations against real information, it is hard to be absolutely certain it is a completely accurate representation.
\subsection{Symbol Recovery}
The key uncertainty with this is that the interference can cause slight shifts in the phase change, as previously mentioned this means that a threshold has to be used when looking for these peaks. In this instance of continuous interference, the length of impulse can be similar to $T_b$ therefore in a prolonged linear progression of the phase this can appear as a bit transition. High amplitude shorter impulses tend to cause a 'jitter' which can result in very high frequency components presenting in the differential.

\section{Wider Implications}
The results of this investigation use a simpler method of bit extraction compared to a lot of the literature and achieve similar results, when relating directly to the real data in particular, \cite{Yang2016}. In communications normalised SNR is a typical comparison metric; in this investigation it is shown that a more localised metric is required for long bit sequences as lightning is typically short lived and highly impulsive therefore over a prolonged time period it is likely to have very little impact on the overall SNR. 
\\\\
In the context of practical VLF communication with potential for high BER, there is a requirement for more complex encryption methods in order to account for possible bit loss. In simple terms high bit error rate results in the requirement for more data to be transmitted. Therefore the ability to recover more of the bit structure from the received signal using non-data related methods to reduce bit errors has a twofold advantage of allowing the complexity of encryption to be reducing the amount of data needing to be transmitted. Reducing the amount of data to be received with a robust reception method inherently reduces the chance of a bit error. The other advantage is the operational advantage to the submarines, as currently the error correction method is implicit as the transmitters repeat the message constantly. Although it is difficult to eliminate this process on the basis that due to operational constraints there may be periods of time where a submarine cannot be at the appropriate depth to receive communications. However, a robust receiver will prevent the submarine from having to remain shallow for any longer than necessary. Long messages take a long time to transmit which could require the submarine to remain shallow for three or four times longer in order to ensure that they have received a complete transmission.